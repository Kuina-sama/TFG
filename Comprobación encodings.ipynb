{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(r\"C:\\Users\\kuina\\OneDrive\\TFG\\Codigo\\CoDeLin\")\n",
    "# from CoDeLin.models.conll_node import ConllNode\n",
    "from models.conll_node import ConllNode\n",
    "from CoDeLin.encs.enc_deps import *\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\kuina\\OneDrive\\TFG\\Codigo\\Datasets\\ConvAI2\\convai2_val.json','r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i,sentence in enumerate(data):\n",
    "    if i ==1:\n",
    "        break\n",
    "    test.append(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello what are doing today ?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en',processors='tokenize,mwt,pos,lemma,depparse',verbose=False, tokenize_no_ssplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando parsing de dependencias....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso terminado con éxito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder = naive_absolute.D_NaiveAbsoluteEncoding('_')\n",
    "\n",
    "print('Comenzando parsing de dependencias....\\n')\n",
    "\n",
    "docs_in = [stanza.Document([], text = doc) for doc in test]\n",
    "\n",
    "docs_out = nlp(docs_in)\n",
    "\n",
    "\n",
    "for doc in tqdm(docs_out):\n",
    "\n",
    "\n",
    "    dicts = doc.to_dict()\n",
    "    conllu_nodes = []\n",
    "    conllu_nodes.append(ConllNode.dummy_root())\n",
    "    for item in dicts[0]:\n",
    "        id = item.get('id','_')\n",
    "        form = item.get('text','_')\n",
    "        # lemma =  item.get('lemma','_')\n",
    "        upos =  item.get('upos','_')\n",
    "        # xpos = item.get('xpos','_')\n",
    "        # feats = item.get('feats','_')\n",
    "        head = item.get('head','_')\n",
    "        deprel = item.get('deprel','_')\n",
    "        \n",
    "        conllu_nodes.append(ConllNode(wid = id, form = form, lemma =  '_', upos =  upos, xpos= '_', \n",
    "            feats = '_', head= head, deprel =  deprel, deps = '_', misc = '_'))\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "        # dataset[doc.text][type] = [str(label) for label in encoders[type].encode(conllu_nodes)]\n",
    "    labels = encoder.encode(conllu_nodes)\n",
    "\n",
    "\n",
    "print('Proceso terminado con éxito')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0_root, 4_obj, 4_aux, 1_parataxis, 4_obl:tmod, 1_punct]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "postags = []\n",
    "\n",
    "for item in conllu_nodes[1:]:\n",
    "    words.append(item.form)\n",
    "    postags.append(item.upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = encoder.decode(labels,postags,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded == conllu_nodes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_equal(nodes1,nodes2):\n",
    "    for node in nodes1:\n",
    "        id = node.id\n",
    "        if node.form != nodes2[id-1].form:\n",
    "            return False\n",
    "        elif node.upos != nodes2[id-1].upos:\n",
    "            return False\n",
    "        elif node.head != nodes2[id-1].head:\n",
    "            return False\n",
    "        elif node.relation != nodes2[id-1].relation:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_equal(conllu_nodes[1:],decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_encoding(conllu_nodes,labels,encoder):\n",
    "    words = []\n",
    "    postags = []\n",
    "\n",
    "    for item in conllu_nodes[1:]:\n",
    "        words.append(item.form)\n",
    "        postags.append(item.upos)\n",
    "\n",
    "    decoded = encoder.decode(labels,postags,words)\n",
    "\n",
    "    return is_equal(conllu_nodes[1:],decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_encoding(conllu_nodes,labels,encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compruebo con más oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i,sentence in enumerate(data):\n",
    "    if i ==10:\n",
    "        break\n",
    "    test.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(data.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando parsing de dependencias....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7503/7503 [00:00<00:00, 14976.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso terminado con éxito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = naive_absolute.D_NaiveAbsoluteEncoding('_')\n",
    "\n",
    "print('Comenzando parsing de dependencias....\\n')\n",
    "\n",
    "docs_in = [stanza.Document([], text = doc) for doc in test]\n",
    "\n",
    "docs_out = nlp(docs_in)\n",
    "\n",
    "errors = 0\n",
    "for doc in tqdm(docs_out):\n",
    "\n",
    "\n",
    "    dicts = doc.to_dict()\n",
    "    conllu_nodes = []\n",
    "    conllu_nodes.append(ConllNode.dummy_root())\n",
    "    for item in dicts[0]:\n",
    "        id = item.get('id','_')\n",
    "        form = item.get('text','_')\n",
    "        # lemma =  item.get('lemma','_')\n",
    "        upos =  item.get('upos','_')\n",
    "        # xpos = item.get('xpos','_')\n",
    "        # feats = item.get('feats','_')\n",
    "        head = item.get('head','_')\n",
    "        deprel = item.get('deprel','_')\n",
    "        \n",
    "        conllu_nodes.append(ConllNode(wid = id, form = form, lemma =  '_', upos =  upos, xpos= '_', \n",
    "            feats = '_', head= head, deprel =  deprel, deps = '_', misc = '_'))\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "        # dataset[doc.text][type] = [str(label) for label in encoders[type].encode(conllu_nodes)]\n",
    "    labels = encoder.encode(conllu_nodes)\n",
    "\n",
    "    if not check_encoding(conllu_nodes,labels,encoder):\n",
    "        errors +=1\n",
    "\n",
    "print('Proceso terminado con éxito')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando parsing de dependencias....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7503/7503 [00:00<00:00, 14373.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso terminado con éxito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = naive_relative.D_NaiveRelativeEncoding('_')\n",
    "\n",
    "print('Comenzando parsing de dependencias....\\n')\n",
    "\n",
    "docs_in = [stanza.Document([], text = doc) for doc in test]\n",
    "\n",
    "docs_out = nlp(docs_in)\n",
    "\n",
    "errors = 0\n",
    "for doc in tqdm(docs_out):\n",
    "\n",
    "\n",
    "    dicts = doc.to_dict()\n",
    "    conllu_nodes = []\n",
    "    conllu_nodes.append(ConllNode.dummy_root())\n",
    "    for item in dicts[0]:\n",
    "        id = item.get('id','_')\n",
    "        form = item.get('text','_')\n",
    "        # lemma =  item.get('lemma','_')\n",
    "        upos =  item.get('upos','_')\n",
    "        # xpos = item.get('xpos','_')\n",
    "        # feats = item.get('feats','_')\n",
    "        head = item.get('head','_')\n",
    "        deprel = item.get('deprel','_')\n",
    "        \n",
    "        conllu_nodes.append(ConllNode(wid = id, form = form, lemma =  '_', upos =  upos, xpos= '_', \n",
    "            feats = '_', head= head, deprel =  deprel, deps = '_', misc = '_'))\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "        # dataset[doc.text][type] = [str(label) for label in encoders[type].encode(conllu_nodes)]\n",
    "    labels = encoder.encode(conllu_nodes)\n",
    "\n",
    "    if not check_encoding(conllu_nodes,labels,encoder):\n",
    "        errors +=1\n",
    "\n",
    "print('Proceso terminado con éxito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS BASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando parsing de dependencias....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7503/7503 [00:00<00:00, 11668.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso terminado con éxito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = pos_based.D_PosBasedEncoding('_')\n",
    "\n",
    "print('Comenzando parsing de dependencias....\\n')\n",
    "\n",
    "docs_in = [stanza.Document([], text = doc) for doc in test]\n",
    "\n",
    "docs_out = nlp(docs_in)\n",
    "\n",
    "errors = 0\n",
    "errors_sentences = []\n",
    "for doc in tqdm(docs_out):\n",
    "\n",
    "\n",
    "    dicts = doc.to_dict()\n",
    "    conllu_nodes = []\n",
    "    conllu_nodes.append(ConllNode.dummy_root())\n",
    "    for item in dicts[0]:\n",
    "        id = item.get('id','_')\n",
    "        form = item.get('text','_')\n",
    "        # lemma =  item.get('lemma','_')\n",
    "        upos =  item.get('upos','_')\n",
    "        # xpos = item.get('xpos','_')\n",
    "        # feats = item.get('feats','_')\n",
    "        head = item.get('head','_')\n",
    "        deprel = item.get('deprel','_')\n",
    "        \n",
    "        conllu_nodes.append(ConllNode(wid = id, form = form, lemma =  '_', upos =  upos, xpos= '_', \n",
    "            feats = '_', head= head, deprel =  deprel, deps = '_', misc = '_'))\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "        # dataset[doc.text][type] = [str(label) for label in encoders[type].encode(conllu_nodes)]\n",
    "    labels = encoder.encode(conllu_nodes)\n",
    "\n",
    "    if not check_encoding(conllu_nodes,labels,encoder):\n",
    "        errors +=1\n",
    "        errors_sentences.append(doc.text)\n",
    "\n",
    "print('Proceso terminado con éxito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['understood . when i need to be alone , i work out a few times each week .',\n",
       " 'bummer ! does your ex like mexican food and fast cars ? she could be my stepmom .',\n",
       " 'how is the weather there today ? it is cloudy and cool here in boston .',\n",
       " 'oh right how i am between jobs',\n",
       " \"i'm not there yet . my oldest in a junior .\",\n",
       " 'kids these days . scorpions are the only thing i ever hear about while coaching .',\n",
       " 'oh wow that cool , you get free knowlege',\n",
       " \"i've powers too as an animal activist !\",\n",
       " 'females rock . my armpits are very hairy .',\n",
       " 'born that way . had to have a transplant , but it seems ok now .',\n",
       " 'very much ! and my husband is a doctor , so he is never home .',\n",
       " 'bar work can be fun . you learn so much about human nature . thank you for caring about others',\n",
       " 'paella is my favorite dish to cook .',\n",
       " 'row row row your boat gently down the stream , when you see a crocodile . . .',\n",
       " 'how long are you married ? did you go to college ?',\n",
       " 'tacos are my favorite , do you have any great food trucks there ?',\n",
       " \"surprisingly enough , too , i've never broken a bone . tell me something else about yourself .\",\n",
       " 'britney spears and her old clothing',\n",
       " 'summer is my favorite season .',\n",
       " 'springer spaniel and a labrador retriever .',\n",
       " 'working on them ? that sounds painful !',\n",
       " 'how long have you got ?',\n",
       " 'talking about fish chips is making me hungry for fish tacos !',\n",
       " \"i've been to spain a few times , where do you go for thanksgiving ?\",\n",
       " 'oh no ! but you still have to love grandma lol',\n",
       " '45 m chicago in town . just trying to stay away from the bars',\n",
       " \"that's what everyone should strive for\",\n",
       " 'horses can be a lot of hard work . i am retired , trying to avoid work .',\n",
       " 'thanksgiving is right around the corner i am very sad about it !',\n",
       " 'traveling , exciting ! do you have any pets ?',\n",
       " 'singing and cleaning . my grandpa and i served in the world war .',\n",
       " 'ambitious . i am just about fluent in spanish . i am starting french soon .',\n",
       " 'rich is probably nice . i would like more money for travel',\n",
       " \"looking over the little league roster . i've to coach in the morning .\",\n",
       " \"really why is that ? mine doesn't get me for being a vegetarian\",\n",
       " 'folk music and i like to play the piano and guitar as well .',\n",
       " 'oh ok . whats your occupation',\n",
       " 'name 3 country songs you like',\n",
       " 'good tired from walking dogs early in the morning !',\n",
       " 'spiderman and gas stations rule',\n",
       " \"so far , they're new to me so we will see . married or kids ?\",\n",
       " 'never to late , you are a catch ! look at it that way !',\n",
       " 'howdy , haw are you this morning ?',\n",
       " 'i wanna be like you some day .',\n",
       " \"taking some time off , i had a plan and i can't follow through\",\n",
       " 'obesity is a state of mind , not of body',\n",
       " 'scorpions and spiders are my number one fear in regards to ways to die .',\n",
       " 'how is it there ? how long have you been there ?',\n",
       " 'ba dum tss ! pizza is so delicious .',\n",
       " \"good . i'm ready for winter . it is so hot here in alabama\",\n",
       " \"hi i'm danny and want to be a dog when i grow up\",\n",
       " 'me to . by the way , what are your dreams .',\n",
       " 'hip hop get down and hip in the times man old school real stuff',\n",
       " 'sanitation department , nothing special , i got in because my grandfather was in world war 2',\n",
       " 'quite often i find it a good source of meditation . do you meditate ?',\n",
       " \"that's kind of you . do you have any pets ?\",\n",
       " 'pot farm updates ? like a real pot farm ?',\n",
       " 'cleaning house must be a great workout . i love working out several times a week',\n",
       " 'folk is my favorite type of music . what is yours ?',\n",
       " 'video games usually . you live in united states all your life ?',\n",
       " \"oh , well i've parents , and a brother that's a little older than me\",\n",
       " 'chemistry was my favorite subject , did you get bullied at all ?',\n",
       " \"where are you from ? i'm from where my favorite rapper is from .\",\n",
       " 'teaching is a great profession , i think . do you have any hobbies ?',\n",
       " 'drama action filled and comedy type of movies',\n",
       " 'star wars or the crap we have to live with in todays society',\n",
       " 'eating out , have to pick my sister up after she gets off work dancing at the club',\n",
       " \"working on a farm , i've to stay active . i work out regularly .\",\n",
       " 'i am from the us',\n",
       " \"nice ! that's really neat !\",\n",
       " '1 2 3 blocks do you like candy',\n",
       " 'right on ! do you like country music ? i love it .',\n",
       " 'college is the place to figure that out',\n",
       " 'burger king is a fun job . i have been studying language all day !',\n",
       " \"howdy , i'm george . how are you ?\",\n",
       " 'how long have you been a doctor ?',\n",
       " 'that is very kind of you .',\n",
       " 'how how are you tonight ?',\n",
       " 'good for you , some days i feel old after riding a roller coaster i can be stiff .',\n",
       " 'how long have you been working as a gamer ?',\n",
       " 'man one time i had a cat and he swallowed another cat whole',\n",
       " 'finance is an awesome career . what are you looking to do ?',\n",
       " \"very well thank you . i'm fluent in several languages , what is your main language ?\",\n",
       " 'how often do you ride a horse ?',\n",
       " \"hi i'm george how are you ?\",\n",
       " 'trying to have body heal itself . wish medical insurance covered it .',\n",
       " 'farm work is hard work from what i understand .',\n",
       " 'how often do you get to ride your horses ?',\n",
       " \"racing . i've a red and blue helmet that matches my brown hair .\",\n",
       " 'dude , the band man . anyway i would rather larp than paint my face like a rocker',\n",
       " 'how long has he been in',\n",
       " 'understandable . life is too short to not follow your dreams . go for it , girl !',\n",
       " \"detective work . i'm a medium and want to lend my services to solve crimes .\",\n",
       " 'it was just me and my mom growing up and she liked coffee .',\n",
       " 'neat . what is your family like ?',\n",
       " 'softball is probably my favorite hobby .',\n",
       " 'no . haha . mostly in a baseball diamond .',\n",
       " 'running . cricket reminds me of baseball .',\n",
       " 'sup man how are you today',\n",
       " 'right now i should be learning how to survive winters here . not used to cold .',\n",
       " 'congrats . big accomplishment . that is awesome',\n",
       " 'hero is also spiderman and villian the joker',\n",
       " 'doing great . do you have any hobbies you are fond of ?',\n",
       " \"country music doesn't fit my demographic 23 year old spaghetti enthusiast .\",\n",
       " 'picking up my toys . mom yelled at me .',\n",
       " \"no i'm stuck here for shoplifting , my mom at least is visiting me tomorrow .\",\n",
       " 'playing volleyball i love it . what about you ?',\n",
       " \"right now i'm a janitor . it pays my bills but that's it\",\n",
       " 'understood . what kind of music do you like ?',\n",
       " 'right now i just live with my cat spook .',\n",
       " 'doing well . i just moved to this country and am learning new things .',\n",
       " 'hamburgers are my weakness ! they stop my powers from working .',\n",
       " 'pizza is my favorite . especially with black olives . sushi is my other favorite !',\n",
       " 'its what i fear . my father is a nascar drive',\n",
       " 'watching the cubs now ! you like the cubs ?',\n",
       " 'how long have you worked there',\n",
       " 'it is , just me and my dogs .',\n",
       " \"well i'm 56 , so i don't live at home anymore .\",\n",
       " 'west virginia . nice to meet you too . i used to visit my uncle on long island',\n",
       " 'computer games til i die . what are people ?',\n",
       " 'how long have you been eating kosher ?',\n",
       " 'how long have you been there',\n",
       " 'me too . what is your favorite gym activity ?',\n",
       " 'gross . that is just not right',\n",
       " \"no i'm not . i'm in real life a music teacher\",\n",
       " 'action and history mostly , the great escape is my favorite',\n",
       " 'school systems are a failing institution anyways . at least your making a difference in the army !',\n",
       " 'reading is a good hobby , i remember when no one had a t . v .',\n",
       " 'working at a gas station is not going to make me rich .',\n",
       " 'how was the food there ?',\n",
       " 'video games are ok . i prefer facebook .',\n",
       " 'pretty much the same , things are looking up though , what do you do ?',\n",
       " '100k is very impressive . how long did it take to get that many ?',\n",
       " 'trooper is my sons name',\n",
       " 'cool cool . i had a 69 chevelle once .',\n",
       " 'doing good went to watch my friends at the skate park today',\n",
       " 'how long have you had him',\n",
       " \"dad is military , mom is dead and i'm tracy . hi !\",\n",
       " 'flamingo dancing is not stripping , think of tango .',\n",
       " \"yes , i'm 13 and i've an older brother .\",\n",
       " 'mood booster in a way . i like to skateboard . do you ?',\n",
       " 'doing good ! i love to skateboard !',\n",
       " \"i'm more excited to go to the playground\",\n",
       " 'oh okay because you are a delivery nurse',\n",
       " 'country music is nice what your favorite song',\n",
       " 'keeps for a busy life does not it ?',\n",
       " 'very well thank you . do you have any hobbies ?',\n",
       " 'doing great . daddy was in the army , he taught me things .',\n",
       " 'visiting the city and someone recommended it .',\n",
       " 'adult coloring books are all the rage']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ['understood . when i need to be alone , i work out a few times each week .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando parsing de dependencias....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 999.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso terminado con éxito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = pos_based.D_PosBasedEncoding('_')\n",
    "\n",
    "print('Comenzando parsing de dependencias....\\n')\n",
    "\n",
    "docs_in = [stanza.Document([], text = doc) for doc in s]\n",
    "\n",
    "docs_out = nlp(docs_in)\n",
    "\n",
    "\n",
    "for doc in tqdm(docs_out):\n",
    "\n",
    "\n",
    "    dicts = doc.to_dict()\n",
    "    conllu_nodes = []\n",
    "    conllu_nodes.append(ConllNode.dummy_root())\n",
    "    for item in dicts[0]:\n",
    "        id = item.get('id','_')\n",
    "        form = item.get('text','_')\n",
    "        upos =  item.get('upos','_')\n",
    "\n",
    "        head = item.get('head','_')\n",
    "        deprel = item.get('deprel','_')\n",
    "        \n",
    "        conllu_nodes.append(ConllNode(wid = id, form = form, lemma =  '_', upos =  upos, xpos= '_', \n",
    "            feats = '_', head= head, deprel =  deprel, deps = '_', misc = '_'))\n",
    "    \n",
    "\n",
    "    labels = encoder.encode(conllu_nodes)\n",
    "\n",
    "\n",
    "\n",
    "print('Proceso terminado con éxito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0\t-ROOT-\t_\t-ROOT-\t_\t_\t0\t-NOREL-\t_\t_,\n",
       " 1\tunderstood\t_\tVERB\t_\t_\t11\tadvcl\t_\t_,\n",
       " 2\t.\t_\tPUNCT\t_\t_\t1\tpunct\t_\t_,\n",
       " 3\twhen\t_\tSCONJ\t_\t_\t5\tmark\t_\t_,\n",
       " 4\ti\t_\tPRON\t_\t_\t5\tnsubj\t_\t_,\n",
       " 5\tneed\t_\tVERB\t_\t_\t11\tadvcl\t_\t_,\n",
       " 6\tto\t_\tPART\t_\t_\t8\tmark\t_\t_,\n",
       " 7\tbe\t_\tAUX\t_\t_\t8\tcop\t_\t_,\n",
       " 8\talone\t_\tADJ\t_\t_\t5\txcomp\t_\t_,\n",
       " 9\t,\t_\tPUNCT\t_\t_\t11\tpunct\t_\t_,\n",
       " 10\ti\t_\tPRON\t_\t_\t11\tnsubj\t_\t_,\n",
       " 11\twork\t_\tVERB\t_\t_\t0\troot\t_\t_,\n",
       " 12\tout\t_\tADP\t_\t_\t11\tcompound:prt\t_\t_,\n",
       " 13\ta\t_\tDET\t_\t_\t15\tdet\t_\t_,\n",
       " 14\tfew\t_\tADJ\t_\t_\t15\tamod\t_\t_,\n",
       " 15\ttimes\t_\tNOUN\t_\t_\t11\tobl:tmod\t_\t_,\n",
       " 16\teach\t_\tDET\t_\t_\t17\tdet\t_\t_,\n",
       " 17\tweek\t_\tNOUN\t_\t_\t11\tobl:tmod\t_\t_,\n",
       " 18\t.\t_\tPUNCT\t_\t_\t11\tpunct\t_\t_]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conllu_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "postags = []\n",
    "\n",
    "for item in conllu_nodes[1:]:\n",
    "    words.append(item.form)\n",
    "    postags.append(item.upos)\n",
    "\n",
    "decoded = encoder.decode(labels,postags,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1\tunderstood\t_\tVERB\t_\t_\t5\tadvcl\t_\t_,\n",
       " 2\t.\t_\tPUNCT\t_\t_\t1\tpunct\t_\t_,\n",
       " 3\twhen\t_\tSCONJ\t_\t_\t5\tmark\t_\t_,\n",
       " 4\ti\t_\tPRON\t_\t_\t5\tnsubj\t_\t_,\n",
       " 5\tneed\t_\tVERB\t_\t_\t5\tadvcl\t_\t_,\n",
       " 6\tto\t_\tPART\t_\t_\t8\tmark\t_\t_,\n",
       " 7\tbe\t_\tAUX\t_\t_\t8\tcop\t_\t_,\n",
       " 8\talone\t_\tADJ\t_\t_\t5\txcomp\t_\t_,\n",
       " 9\t,\t_\tPUNCT\t_\t_\t11\tpunct\t_\t_,\n",
       " 10\ti\t_\tPRON\t_\t_\t11\tnsubj\t_\t_,\n",
       " 11\twork\t_\tVERB\t_\t_\t0\troot\t_\t_,\n",
       " 12\tout\t_\tADP\t_\t_\t11\tcompound:prt\t_\t_,\n",
       " 13\ta\t_\tDET\t_\t_\t15\tdet\t_\t_,\n",
       " 14\tfew\t_\tADJ\t_\t_\t15\tamod\t_\t_,\n",
       " 15\ttimes\t_\tNOUN\t_\t_\t11\tobl:tmod\t_\t_,\n",
       " 16\teach\t_\tDET\t_\t_\t17\tdet\t_\t_,\n",
       " 17\tweek\t_\tNOUN\t_\t_\t11\tobl:tmod\t_\t_,\n",
       " 18\t.\t_\tPUNCT\t_\t_\t11\tpunct\t_\t_]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRK based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando parsing de dependencias....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7503/7503 [00:00<00:00, 12236.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso terminado con éxito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = brk_based.D_BrkBasedEncoding('_',True)\n",
    "\n",
    "print('Comenzando parsing de dependencias....\\n')\n",
    "\n",
    "docs_in = [stanza.Document([], text = doc) for doc in test]\n",
    "\n",
    "docs_out = nlp(docs_in)\n",
    "\n",
    "errors = 0\n",
    "errors_sentences = []\n",
    "for doc in tqdm(docs_out):\n",
    "\n",
    "\n",
    "    dicts = doc.to_dict()\n",
    "    conllu_nodes = []\n",
    "    conllu_nodes.append(ConllNode.dummy_root())\n",
    "    for item in dicts[0]:\n",
    "        id = item.get('id','_')\n",
    "        form = item.get('text','_')\n",
    "        # lemma =  item.get('lemma','_')\n",
    "        upos =  item.get('upos','_')\n",
    "        # xpos = item.get('xpos','_')\n",
    "        # feats = item.get('feats','_')\n",
    "        head = item.get('head','_')\n",
    "        deprel = item.get('deprel','_')\n",
    "        \n",
    "        conllu_nodes.append(ConllNode(wid = id, form = form, lemma =  '_', upos =  upos, xpos= '_', \n",
    "            feats = '_', head= head, deprel =  deprel, deps = '_', misc = '_'))\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "        # dataset[doc.text][type] = [str(label) for label in encoders[type].encode(conllu_nodes)]\n",
    "    labels = encoder.encode(conllu_nodes)\n",
    "\n",
    "    if not check_encoding(conllu_nodes,labels,encoder):\n",
    "        errors +=1\n",
    "        errors_sentences.append(conllu_nodes)\n",
    "\n",
    "print('Proceso terminado con éxito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0\t-ROOT-\t_\t-ROOT-\t_\t_\t0\t-NOREL-\t_\t_,\n",
       "  1\tso\t_\tADV\t_\t_\t6\tadvmod\t_\t_,\n",
       "  2\twhat\t_\tPRON\t_\t_\t8\tobj\t_\t_,\n",
       "  3\telse\t_\tADV\t_\t_\t2\tadvmod\t_\t_,\n",
       "  4\tdo\t_\tAUX\t_\t_\t6\taux\t_\t_,\n",
       "  5\tyou\t_\tPRON\t_\t_\t6\tnsubj\t_\t_,\n",
       "  6\tlike\t_\tVERB\t_\t_\t0\troot\t_\t_,\n",
       "  7\tto\t_\tPART\t_\t_\t8\tmark\t_\t_,\n",
       "  8\tdo\t_\tVERB\t_\t_\t6\txcomp\t_\t_,\n",
       "  9\ton\t_\tADP\t_\t_\t12\tcase\t_\t_,\n",
       "  10\tyour\t_\tPRON\t_\t_\t12\tnmod:poss\t_\t_,\n",
       "  11\tspare\t_\tADJ\t_\t_\t12\tamod\t_\t_,\n",
       "  12\ttime\t_\tNOUN\t_\t_\t8\tobl\t_\t_,\n",
       "  13\t?\t_\tPUNCT\t_\t_\t6\tpunct\t_\t_],\n",
       " [0\t-ROOT-\t_\t-ROOT-\t_\t_\t0\t-NOREL-\t_\t_,\n",
       "  1\tits\t_\tPRON\t_\t_\t5\tnmod:poss\t_\t_,\n",
       "  2\ttotally\t_\tADV\t_\t_\t3\tadvmod\t_\t_,\n",
       "  3\twrong\t_\tADJ\t_\t_\t8\tnsubj\t_\t_,\n",
       "  4\tits\t_\tPRON\t_\t_\t5\tnmod:poss\t_\t_,\n",
       "  5\tending\t_\tNOUN\t_\t_\t8\tnsubj\t_\t_,\n",
       "  6\twill\t_\tAUX\t_\t_\t8\taux\t_\t_,\n",
       "  7\tbe\t_\tAUX\t_\t_\t8\tcop\t_\t_,\n",
       "  8\tworst\t_\tADJ\t_\t_\t0\troot\t_\t_,\n",
       "  9\tthen\t_\tADV\t_\t_\t10\tadvmod\t_\t_,\n",
       "  10\tever\t_\tADV\t_\t_\t8\tadvmod\t_\t_],\n",
       " [0\t-ROOT-\t_\t-ROOT-\t_\t_\t0\t-NOREL-\t_\t_,\n",
       "  1\twhat\t_\tPRON\t_\t_\t6\tobj\t_\t_,\n",
       "  2\tdo\t_\tAUX\t_\t_\t4\taux\t_\t_,\n",
       "  3\tyou\t_\tPRON\t_\t_\t4\tnsubj\t_\t_,\n",
       "  4\tlike\t_\tVERB\t_\t_\t9\tparataxis\t_\t_,\n",
       "  5\tto\t_\tPART\t_\t_\t6\tmark\t_\t_,\n",
       "  6\teat\t_\tVERB\t_\t_\t4\txcomp\t_\t_,\n",
       "  7\t?\t_\tPUNCT\t_\t_\t4\tpunct\t_\t_,\n",
       "  8\ti\t_\tPRON\t_\t_\t9\tnsubj\t_\t_,\n",
       "  9\tdiet\t_\tVERB\t_\t_\t0\troot\t_\t_,\n",
       "  10\tall\t_\tDET\t_\t_\t12\tdet:predet\t_\t_,\n",
       "  11\tthe\t_\tDET\t_\t_\t12\tdet\t_\t_,\n",
       "  12\ttime\t_\tNOUN\t_\t_\t9\tobl:tmod\t_\t_,\n",
       "  13\t.\t_\tPUNCT\t_\t_\t9\tpunct\t_\t_],\n",
       " [0\t-ROOT-\t_\t-ROOT-\t_\t_\t0\t-NOREL-\t_\t_,\n",
       "  1\thi\t_\tINTJ\t_\t_\t6\tdiscourse\t_\t_,\n",
       "  2\t,\t_\tPUNCT\t_\t_\t1\tpunct\t_\t_,\n",
       "  3\twhat\t_\tPRON\t_\t_\t8\tobj\t_\t_,\n",
       "  4\tdo\t_\tAUX\t_\t_\t6\taux\t_\t_,\n",
       "  5\tyou\t_\tPRON\t_\t_\t6\tnsubj\t_\t_,\n",
       "  6\tlike\t_\tVERB\t_\t_\t11\tparataxis\t_\t_,\n",
       "  7\tto\t_\tPART\t_\t_\t8\tmark\t_\t_,\n",
       "  8\teat\t_\tVERB\t_\t_\t6\txcomp\t_\t_,\n",
       "  9\t?\t_\tPUNCT\t_\t_\t6\tpunct\t_\t_,\n",
       "  10\ti\t_\tPRON\t_\t_\t11\tnsubj\t_\t_,\n",
       "  11\tprefer\t_\tVERB\t_\t_\t0\troot\t_\t_,\n",
       "  12\ta\t_\tDET\t_\t_\t14\tdet\t_\t_,\n",
       "  13\traw\t_\tADJ\t_\t_\t14\tamod\t_\t_,\n",
       "  14\tdiet\t_\tNOUN\t_\t_\t11\tobj\t_\t_,\n",
       "  15\t.\t_\tPUNCT\t_\t_\t11\tpunct\t_\t_],\n",
       " [0\t-ROOT-\t_\t-ROOT-\t_\t_\t0\t-NOREL-\t_\t_,\n",
       "  1\twell\t_\tINTJ\t_\t_\t7\tdiscourse\t_\t_,\n",
       "  2\tnot\t_\tPART\t_\t_\t3\tadvmod\t_\t_,\n",
       "  3\tmuch\t_\tADV\t_\t_\t8\tadvmod\t_\t_,\n",
       "  4\tjust\t_\tADV\t_\t_\t7\tadvmod\t_\t_,\n",
       "  5\ta\t_\tDET\t_\t_\t7\tdet\t_\t_,\n",
       "  6\tretired\t_\tVERB\t_\t_\t7\tamod\t_\t_,\n",
       "  7\tveteran\t_\tNOUN\t_\t_\t0\troot\t_\t_,\n",
       "  8\taddicted\t_\tADJ\t_\t_\t7\tamod\t_\t_,\n",
       "  9\tto\t_\tADP\t_\t_\t10\tcase\t_\t_,\n",
       "  10\tgambling\t_\tNOUN\t_\t_\t8\tobl\t_\t_,\n",
       "  11\t.\t_\tPUNCT\t_\t_\t8\tpunct\t_\t_,\n",
       "  12\tyou\t_\tPRON\t_\t_\t8\tvocative\t_\t_,\n",
       "  13\t?\t_\tPUNCT\t_\t_\t7\tpunct\t_\t_],\n",
       " [0\t-ROOT-\t_\t-ROOT-\t_\t_\t0\t-NOREL-\t_\t_,\n",
       "  1\toh\t_\tINTJ\t_\t_\t4\tdiscourse\t_\t_,\n",
       "  2\tthere\t_\tADV\t_\t_\t9\tadvmod\t_\t_,\n",
       "  3\tyou\t_\tPRON\t_\t_\t4\tnsubj\t_\t_,\n",
       "  4\tgo\t_\tVERB\t_\t_\t9\treparandum\t_\t_,\n",
       "  5\t.\t_\tPUNCT\t_\t_\t4\tpunct\t_\t_,\n",
       "  6\tthat\t_\tDET\t_\t_\t7\tdet\t_\t_,\n",
       "  7\tcolor\t_\tNOUN\t_\t_\t9\tnsubj\t_\t_,\n",
       "  8\tcould\t_\tAUX\t_\t_\t9\taux\t_\t_,\n",
       "  9\tgo\t_\tVERB\t_\t_\t0\troot\t_\t_,\n",
       "  10\teither\t_\tDET\t_\t_\t11\tdet\t_\t_,\n",
       "  11\tway\t_\tNOUN\t_\t_\t9\tobl:npmod\t_\t_,\n",
       "  12\t.\t_\tPUNCT\t_\t_\t9\tpunct\t_\t_],\n",
       " [0\t-ROOT-\t_\t-ROOT-\t_\t_\t0\t-NOREL-\t_\t_,\n",
       "  1\thi\t_\tINTJ\t_\t_\t6\tdiscourse\t_\t_,\n",
       "  2\t,\t_\tPUNCT\t_\t_\t1\tpunct\t_\t_,\n",
       "  3\twhat\t_\tPRON\t_\t_\t8\tobj\t_\t_,\n",
       "  4\tdo\t_\tAUX\t_\t_\t6\taux\t_\t_,\n",
       "  5\tyou\t_\tPRON\t_\t_\t6\tnsubj\t_\t_,\n",
       "  6\tlike\t_\tVERB\t_\t_\t0\troot\t_\t_,\n",
       "  7\tto\t_\tPART\t_\t_\t8\tmark\t_\t_,\n",
       "  8\tdo\t_\tVERB\t_\t_\t6\txcomp\t_\t_,\n",
       "  9\t?\t_\tPUNCT\t_\t_\t6\tpunct\t_\t_],\n",
       " [0\t-ROOT-\t_\t-ROOT-\t_\t_\t0\t-NOREL-\t_\t_,\n",
       "  1\tlol\t_\tINTJ\t_\t_\t5\tdiscourse\t_\t_,\n",
       "  2\twhat\t_\tPRON\t_\t_\t7\tobj\t_\t_,\n",
       "  3\tdo\t_\tAUX\t_\t_\t5\taux\t_\t_,\n",
       "  4\tyou\t_\tPRON\t_\t_\t5\tnsubj\t_\t_,\n",
       "  5\tlike\t_\tVERB\t_\t_\t10\treparandum\t_\t_,\n",
       "  6\tto\t_\tPART\t_\t_\t7\tmark\t_\t_,\n",
       "  7\tdo\t_\tVERB\t_\t_\t5\txcomp\t_\t_,\n",
       "  8\t?\t_\tPUNCT\t_\t_\t5\tpunct\t_\t_,\n",
       "  9\ti\t_\tPRON\t_\t_\t10\tnsubj\t_\t_,\n",
       "  10\tlike\t_\tVERB\t_\t_\t0\troot\t_\t_,\n",
       "  11\tspending\t_\tVERB\t_\t_\t10\txcomp\t_\t_,\n",
       "  12\ttime\t_\tNOUN\t_\t_\t11\tobj\t_\t_,\n",
       "  13\tat\t_\tADP\t_\t_\t15\tcase\t_\t_,\n",
       "  14\tthe\t_\tDET\t_\t_\t15\tdet\t_\t_,\n",
       "  15\tplayground\t_\tNOUN\t_\t_\t11\tobl\t_\t_],\n",
       " [0\t-ROOT-\t_\t-ROOT-\t_\t_\t0\t-NOREL-\t_\t_,\n",
       "  1\thello\t_\tINTJ\t_\t_\t6\tdiscourse\t_\t_,\n",
       "  2\t.\t_\tPUNCT\t_\t_\t1\tpunct\t_\t_,\n",
       "  3\twhat\t_\tPRON\t_\t_\t8\tobj\t_\t_,\n",
       "  4\tdo\t_\tAUX\t_\t_\t6\taux\t_\t_,\n",
       "  5\tyou\t_\tPRON\t_\t_\t6\tnsubj\t_\t_,\n",
       "  6\tlike\t_\tVERB\t_\t_\t0\troot\t_\t_,\n",
       "  7\tto\t_\tPART\t_\t_\t8\tmark\t_\t_,\n",
       "  8\tdo\t_\tVERB\t_\t_\t6\txcomp\t_\t_,\n",
       "  9\tin\t_\tADP\t_\t_\t12\tcase\t_\t_,\n",
       "  10\tyour\t_\tPRON\t_\t_\t12\tnmod:poss\t_\t_,\n",
       "  11\tfree\t_\tADJ\t_\t_\t12\tamod\t_\t_,\n",
       "  12\ttime\t_\tNOUN\t_\t_\t8\tobl\t_\t_,\n",
       "  13\t?\t_\tPUNCT\t_\t_\t6\tpunct\t_\t_]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compruebo si las oraciones con errores son proyectivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_projective(arcs: list):\n",
    "    \"\"\"\n",
    "    Determines if a dependency tree has crossing arcs or not.\n",
    "    Parameters:\n",
    "    arcs (list): A list of tuples of the form (headid, dependentid, coding the arcs of the sentence, e.g, [(0,3), (1,4), …]\n",
    "    Returns:\n",
    "    A boolean: True if the tree is projective, False otherwise\n",
    "    \"\"\"\n",
    "    for (i,j) in arcs:\n",
    "        for (k,l) in arcs:\n",
    "            if (i,j) != (k,l) and min(i,j) < min(k,l) < max(i,j) < max(k,l):\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_projective(sentences):\n",
    "    \"\"\"Count's how many non projective sentence are on the sentence lists and returns the total\"\"\"\n",
    "    total = 0\n",
    "    for sentence in sentences:\n",
    "        arcs = []\n",
    "        for token in sentence[1:]:\n",
    "            arcs.append((token.head,token.id))\n",
    "        if not is_projective(arcs):\n",
    "            total +=1\n",
    "    return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_projective(errors_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35e8332e99bdf485583869dfbdef293dcf2f9293b1663ec5daea0a573af457c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
