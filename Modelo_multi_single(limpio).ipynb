{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LibrerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding,AutoModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import AdamW \n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils import train_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones y clases auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map text labels to numeric labels\n",
    "text_to_num = {'to':{'PARTNER:female':0,'PARTNER:male':1,\"PARTNER:unknown\":2},\n",
    "                'as':{'SELF:female':0, 'SELF:male':1,'SELF:unknown':2},\n",
    "                'about':{'ABOUT:female':0,'ABOUT:male':1,'ABOUT:unknown':2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_names = ['about','as','to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset,tasks_names):\n",
    "    token_data = {}\n",
    "    for index, text in enumerate(dataset):\n",
    "        tokenized = tokenizer(text,truncation=True)\n",
    "\n",
    "        labels ={}\n",
    "        for task in tasks_names:\n",
    "            aux_label = [text_to_num[task][x] for x in dataset[text][f'label_{task}']]\n",
    "\n",
    "\n",
    "            labels[task] = aux_label\n",
    "\n",
    "        token_data[index] = {'text':text,\n",
    "                                'input_ids':tokenized.input_ids,\n",
    "                                'attention_mask':tokenized.attention_mask,\n",
    "                                'labels':labels}\n",
    "\n",
    "    return token_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self,complete_data,tasks):\n",
    "        self.data = complete_data\n",
    "        self.tasks = tasks\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "            \n",
    "        x = torch.tensor(self.data[index]['input_ids'])\n",
    "\n",
    "        raw_labels = self.data[index]['labels'] \n",
    "\n",
    "        labels=[]\n",
    "        for task in self.tasks:\n",
    "            aux = raw_labels[task]\n",
    "            if len(aux)>1:\n",
    "                label = np.random.choice(aux)\n",
    "                if label ==2:\n",
    "                    label = np.random.choice([0,1])\n",
    "                labels.append(label)\n",
    "            elif len(aux)==1:\n",
    "                if aux[0] == 2:\n",
    "\n",
    "                    label = np.random.choice([0,1])\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    labels.append(aux[0])\n",
    "\n",
    "\n",
    "\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        attention = torch.tensor(self.data[index]['attention_mask'])\n",
    "\n",
    "\n",
    "        sample = {'input_ids': x,\n",
    "                'attention_mask': attention,\n",
    "                'labels': labels.view(-1,len(labels))}\n",
    "\n",
    "        return  sample\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesado datos ConvAI2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DependencyDataset\\ConvAI2\\convai_rel_complete.json','r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_data = tokenize_dataset(data,tasks_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MyDataSet(complete_data=token_data,tasks=tasks_names)\n",
    "dl_train = DataLoader(dataset_train,batch_size=16,shuffle=True,collate_fn=data_collator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi(nn.Module):\n",
    "    def __init__(self,name,num_labels,tasks_names,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.encoder = AutoModel.from_pretrained(name,num_labels=num_labels,output_attentions=True,output_hidden_states = True)\n",
    "        self.taskLayer = nn.ModuleList([])\n",
    "        self.taskLayer.append(nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(768,num_labels)\n",
    "        ))\n",
    "\n",
    "        # Task 0: About\n",
    "        # Task 1: Self\n",
    "        # Task 2: Partner\n",
    "        \n",
    "        self.task_to_num =  {'about':0,'as':1,'to':2}\n",
    "\n",
    "        \n",
    "        if len(tasks_names) ==1:\n",
    "            self.task = self.task_to_num[tasks_names[0]]\n",
    "            self.SingleTask = True\n",
    "        else:\n",
    "            self.SingleTask=False\n",
    "            self.tasksname = {v:k for v,k in enumerate(tasks_names)}\n",
    "        \n",
    "    def forward(self,input_ids = None,attention_mask = None,labels = None,output_attentions=None,output_hidden_states=None):\n",
    "        \n",
    "\n",
    "\n",
    "        dBertoutputs = self.encoder(input_ids,attention_mask = attention_mask,output_attentions=output_attentions,output_hidden_states = output_hidden_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        outputs_last_hidden_state = dBertoutputs[0]\n",
    "\n",
    "        cls_out = outputs_last_hidden_state[:,0]\n",
    "\n",
    "        if self.SingleTask:\n",
    "            tasks_output = cls_out.clone()\n",
    "            for layer in self.taskLayer:\n",
    "                tasks_output = layer(tasks_output)\n",
    "\n",
    "        else:\n",
    "            tasks_output = {v : cls_out.clone() for v in self.tasksname.keys()}\n",
    "\n",
    "            for layer in self.taskLayer:\n",
    "                tasks_output = {v: layer(k) for v,k in tasks_output.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "            if self.SingleTask:\n",
    "\n",
    "                loss = loss_fct(tasks_output[0] , labels[0][:,self.task].type('torch.FloatTensor').to(device)) \n",
    "            else:\n",
    "                task_loss = [loss_fct(tasks_output[i] , labels[:,0][:,i]) for i in range(len(tasks_output))]\n",
    "                loss = sum(task_loss)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=tasks_output, hidden_states=dBertoutputs.hidden_states,attentions=dBertoutputs.attentions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 3 dimensiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueba que el modelo funcione entrenando con varias dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Multi(\"distilbert-base-uncased\", 2,tasks_names).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de887a17a87848a6aeb48091b95ac6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4588 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 4588/4588 [03:53<00:00, 19.66it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "train_function(model,num_epochs,dl_train,optimizer = AdamW(model.parameters(),lr=5e-5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 1 dimensiÃ³n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueba que se pueda entrenar un modelo SingleTask para la tarea indicada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Multi(\"distilbert-base-uncased\", 1,['to']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbebcc8647d4b1fb63ed1eb980bfffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4588 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 4588/4588 [03:53<00:00, 20.43it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "train_function(model,num_epochs,dl_train,optimizer = AdamW(model.parameters(),lr=5e-5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35e8332e99bdf485583869dfbdef293dcf2f9293b1663ec5daea0a573af457c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
