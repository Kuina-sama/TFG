{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_generic as generic\n",
    "import rrnn_modelo as rrnn\n",
    "\n",
    "\n",
    "\n",
    "import train_datamaps as train\n",
    "from train import eval_func\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "word2vec_path = r\"C:\\Users\\kuina\\gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\"\n",
    "word2vec_model =  KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='w2v'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos ConvAI2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets\\ConvAI2\\convai2_complete.json','r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convai_train = data['train']\n",
    "convai_val = data['validation']\n",
    "\n",
    "# Me quedo solo con el vocabulario que necesito (interseco el de train con el de w2v) y así reduzco la matriz de embeddings\n",
    "vocabulary = generic.create_w2v_vocab(convai_train,word2vec_model)\n",
    "word_to_index = {}\n",
    "emb_weights = []\n",
    "for new_id, word in zip(range(len(vocabulary)),vocabulary):\n",
    "    word_to_index[word] = new_id\n",
    "    emb_weights.append(word2vec_model[word])\n",
    "\n",
    "emb_weights = np.array(emb_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convai_train_token = generic.tokenize_dataset_rrnn(convai_train,['about','to','as'],word_to_index) \n",
    "convai_val_token = generic.tokenize_dataset_rrnn(convai_val,['about','to','as'],word_to_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convai_train_dataset = {}\n",
    "for task in ['to','as','about']:\n",
    "    convai_train_dataset[task] =rrnn.DatasetSingleTaskRRNN(convai_train_token,task,eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convai_val_dataset = {}\n",
    "for task in ['to','as','about']:\n",
    "    convai_val_dataset[task] = rrnn.DatasetSingleTaskRRNN(convai_val_token,task,eval=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos md_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets\\md_gender\\md_complete.json','r',encoding=\"utf8\") as f:\n",
    "    md_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_tokenized = generic.tokenize_dataset_rrnn(md_data,['about','to','as'],word_to_index) \n",
    "\n",
    "\n",
    "md_dataset = {}\n",
    "for task in ['to','as','about']:\n",
    "    md_dataset[task] = rrnn.DatasetSingleTaskRRNN(md_tokenized,task,eval=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = {}\n",
    "for task in ['to','as','about']:\n",
    "    dl_train[task] = DataLoader(convai_train_dataset[task],batch_size=128,shuffle=True,collate_fn=rrnn.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_val = {}\n",
    "for task in ['to','as','about']:\n",
    "    dl_val[task] = DataLoader(convai_val_dataset[task],batch_size=128,shuffle=True,collate_fn=rrnn.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_eval = {}\n",
    "for task in ['to','as','about']:\n",
    "    dl_eval[task] = DataLoader(md_dataset[task],batch_size=128,shuffle=False,collate_fn=rrnn.collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "num_labels = 2\n",
    "lstm_hidden_dim = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "num_epochs = 100\n",
    "task = 'about'\n",
    "\n",
    "global_metrics = {'about':{'recall':{'average':[],'female':[],'male':[]},\n",
    "                            'precision':{'average':[],'female':[],'male':[]},\n",
    "                            'f1':{'average':[],'female':[],'male':[]},\n",
    "                            'acc':[]},\n",
    "                    'to':{'recall':{'average':[],'female':[],'male':[]},\n",
    "                            'precision':{'average':[],'female':[],'male':[]},\n",
    "                            'f1':{'average':[],'female':[],'male':[]},\n",
    "                            'acc':[]},\n",
    "                    'as':{'recall':{'average':[],'female':[],'male':[]},\n",
    "                            'precision':{'average':[],'female':[],'male':[]},\n",
    "                            'f1':{'average':[],'female':[],'male':[]},\n",
    "                            'acc':[]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "save_path=f'm1_{model_name}_simple_{task}_1'\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay=0.1)\n",
    "p, c, epochs = train.train_function(model,num_epochs,dl_train['about'],optimizer,early_stop=10,dl_val=dl_val['about'],save_path=save_path,es_threshold=0)\n",
    "torch.save(p,save_path+'_probs'+'.pt')\n",
    "torch.save(c,save_path+'_corr'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.get_datamap_complete_graph(p,correctness_vector=c,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n')\n",
    "    print(f\"Evaluación en el conjunto el conjunto {task}\")\n",
    "    metric_result = eval_func(model,dl_eval[task])\n",
    "    for metric, value in metric_result.items():\n",
    "        if metric=='accuracy':\n",
    "            global_metrics[task]['acc'].append(value) \n",
    "        else:\n",
    "            for g,v in value.items():\n",
    "                global_metrics[task][metric][g].append(v)\n",
    "\n",
    "        print(metric,metric_result[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "save_path=f'm1_{model_name}_simple_{task}_2'\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay=0.1)\n",
    "p, c, epochs = train.train_function(model,num_epochs,dl_train['about'],optimizer,early_stop=10,dl_val=dl_val['about'],save_path=save_path,es_threshold=0)\n",
    "torch.save(p,save_path+'_probs'+'.pt')\n",
    "torch.save(c,save_path+'_corr'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.get_datamap_complete_graph(p,correctness_vector=c,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n')\n",
    "    print(f\"Evaluación en el conjunto el conjunto {task}\")\n",
    "    metric_result = eval_func(model,dl_eval[task])\n",
    "    for metric, value in metric_result.items():\n",
    "        if metric=='accuracy':\n",
    "            global_metrics[task]['acc'].append(value) \n",
    "        else:\n",
    "            for g,v in value.items():\n",
    "                global_metrics[task][metric][g].append(v)\n",
    "\n",
    "        print(metric,metric_result[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "save_path=f'm1_{model_name}_simple_{task}_3'\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay=0.1)\n",
    "p, c, epochs = train.train_function(model,num_epochs,dl_train['about'],optimizer,early_stop=10,dl_val=dl_val['about'],save_path=save_path,es_threshold=0)\n",
    "torch.save(p,save_path+'_probs'+'.pt')\n",
    "torch.save(c,save_path+'_corr'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.get_datamap_complete_graph(p,correctness_vector=c,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n')\n",
    "    print(f\"Evaluación en el conjunto el conjunto {task}\")\n",
    "    metric_result = eval_func(model,dl_eval[task])\n",
    "    for metric, value in metric_result.items():\n",
    "        if metric=='accuracy':\n",
    "            global_metrics[task]['acc'].append(value) \n",
    "        else:\n",
    "            for g,v in value.items():\n",
    "                global_metrics[task][metric][g].append(v)\n",
    "\n",
    "        print(metric,metric_result[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['f1','recall','precision']:\n",
    "    print(f'{metric} medio de los 3 modelos: \\n')\n",
    "    for task in ['about','to','as']:\n",
    "        print(task.upper())\n",
    "        print(f'Resultado global {metric}:',mean(global_metrics[task][metric]['average']))\n",
    "        print(f'{metric} etiqueta male:',mean(global_metrics[task][metric]['male']))\n",
    "        print(f'{metric} etiqueta female: ',mean(global_metrics[task][metric]['female']))\n",
    "        print('\\n')\n",
    "\n",
    "print(f'Accuracy medio de los 3 modelos: \\n')\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n',task.upper())\n",
    "    print('Resultado global accuracy:',mean(global_metrics[task]['acc']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "num_epochs = 100\n",
    "task = 'to'\n",
    "global_metrics = {'about':{'recall':{'average':[],'female':[],'male':[]},\n",
    "                            'precision':{'average':[],'female':[],'male':[]},\n",
    "                            'f1':{'average':[],'female':[],'male':[]},\n",
    "                            'acc':[]},\n",
    "                    'to':{'recall':{'average':[],'female':[],'male':[]},\n",
    "                            'precision':{'average':[],'female':[],'male':[]},\n",
    "                            'f1':{'average':[],'female':[],'male':[]},\n",
    "                            'acc':[]},\n",
    "                    'as':{'recall':{'average':[],'female':[],'male':[]},\n",
    "                            'precision':{'average':[],'female':[],'male':[]},\n",
    "                            'f1':{'average':[],'female':[],'male':[]},\n",
    "                            'acc':[]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "save_path=f'm1_{model_name}_simple_{task}_1'\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay=0.1)\n",
    "p, c, epochs = train.train_function(model,num_epochs,dl_train[task],optimizer,early_stop=10,dl_val=dl_val[task],save_path=save_path,es_threshold=0)\n",
    "torch.save(p,save_path+'_probs'+'.pt')\n",
    "torch.save(c,save_path+'_corr'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.get_datamap_complete_graph(p,correctness_vector=c,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n')\n",
    "    print(f\"Evaluación en el conjunto el conjunto {task}\")\n",
    "    metric_result = eval_func(model,dl_eval[task])\n",
    "    for metric, value in metric_result.items():\n",
    "        if metric=='accuracy':\n",
    "            global_metrics[task]['acc'].append(value) \n",
    "        else:\n",
    "            for g,v in value.items():\n",
    "                global_metrics[task][metric][g].append(v)\n",
    "\n",
    "        print(metric,metric_result[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "save_path=f'm1_{model_name}_simple_{task}_2'\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay=0.1)\n",
    "p, c, epochs = train.train_function(model,num_epochs,dl_train[task],optimizer,early_stop=10,dl_val=dl_val[task],save_path=save_path,es_threshold=0)\n",
    "torch.save(p,save_path+'_probs'+'.pt')\n",
    "torch.save(p,save_path+'_corr'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.get_datamap_complete_graph(p,correctness_vector=c,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n')\n",
    "    print(f\"Evaluación en el conjunto el conjunto {task}\")\n",
    "    metric_result = eval_func(model,dl_eval[task])\n",
    "    for metric, value in metric_result.items():\n",
    "        if metric=='accuracy':\n",
    "            global_metrics[task]['acc'].append(value) \n",
    "        else:\n",
    "            for g,v in value.items():\n",
    "                global_metrics[task][metric][g].append(v)\n",
    "\n",
    "        print(metric,metric_result[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "save_path=f'm1_{model_name}_simple_{task}_3'\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay=0.1)\n",
    "p, c, epochs = train.train_function(model,num_epochs,dl_train[task],optimizer,early_stop=10,dl_val=dl_val[task],save_path=save_path,es_threshold=0)\n",
    "torch.save(p,save_path+'_probs'+'.pt')\n",
    "torch.save(c,save_path+'_corr'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.get_datamap_complete_graph(p,correctness_vector=c,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n')\n",
    "    print(f\"Evaluación en el conjunto el conjunto {task}\")\n",
    "    metric_result = eval_func(model,dl_eval[task])\n",
    "    for metric, value in metric_result.items():\n",
    "        if metric=='accuracy':\n",
    "            global_metrics[task]['acc'].append(value) \n",
    "        else:\n",
    "            for g,v in value.items():\n",
    "                global_metrics[task][metric][g].append(v)\n",
    "\n",
    "        print(metric,metric_result[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['f1','recall','precision']:\n",
    "    print(f'{metric} medio de los 3 modelos: \\n')\n",
    "    for task in ['about','to','as']:\n",
    "        print(task.upper())\n",
    "        print(f'Resultado global {metric}:',mean(global_metrics[task][metric]['average']))\n",
    "        print(f'{metric} etiqueta male:',mean(global_metrics[task][metric]['male']))\n",
    "        print(f'{metric} etiqueta female: ',mean(global_metrics[task][metric]['female']))\n",
    "        print('\\n')\n",
    "\n",
    "print(f'Accuracy medio de los 3 modelos: \\n')\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n',task.upper())\n",
    "    print('Resultado global accuracy:',mean(global_metrics[task]['acc']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "num_epochs = 100\n",
    "task='as'\n",
    "global_metrics = {'about':{'recall':{'average':[],'female':[],'male':[]},\n",
    "                            'precision':{'average':[],'female':[],'male':[]},\n",
    "                            'f1':{'average':[],'female':[],'male':[]},\n",
    "                            'acc':[]},\n",
    "                    'to':{'recall':{'average':[],'female':[],'male':[]},\n",
    "                            'precision':{'average':[],'female':[],'male':[]},\n",
    "                            'f1':{'average':[],'female':[],'male':[]},\n",
    "                            'acc':[]},\n",
    "                    'as':{'recall':{'average':[],'female':[],'male':[]},\n",
    "                            'precision':{'average':[],'female':[],'male':[]},\n",
    "                            'f1':{'average':[],'female':[],'male':[]},\n",
    "                            'acc':[]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "save_path=f'm1_{model_name}_simple_{task}_1'\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay=0.1)\n",
    "p, c, epochs = train.train_function(model,num_epochs,dl_train[task],optimizer,early_stop=10,dl_val=dl_val[task],save_path=save_path,es_threshold=0)\n",
    "torch.save(p,save_path+'_probs'+'.pt')\n",
    "torch.save(c,save_path+'_corr'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.get_datamap_complete_graph(p,correctness_vector=c,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n')\n",
    "    print(f\"Evaluación en el conjunto el conjunto {task}\")\n",
    "    metric_result = eval_func(model,dl_eval[task])\n",
    "    for metric, value in metric_result.items():\n",
    "        if metric=='accuracy':\n",
    "            global_metrics[task]['acc'].append(value) \n",
    "        else:\n",
    "            for g,v in value.items():\n",
    "                global_metrics[task][metric][g].append(v)\n",
    "\n",
    "        print(metric,metric_result[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "save_path=f'm1_{model_name}_simple_{task}_2'\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay=0.1)\n",
    "p, c, epochs = train.train_function(model,num_epochs,dl_train[task],optimizer,early_stop=10,dl_val=dl_val[task],save_path=save_path,es_threshold=0)\n",
    "torch.save(p,save_path+'_probs'+'.pt')\n",
    "torch.save(c,save_path+'_corr'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.get_datamap_complete_graph(p,correctness_vector=c,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n')\n",
    "    print(f\"Evaluación en el conjunto el conjunto {task}\")\n",
    "    metric_result = eval_func(model,dl_eval[task])\n",
    "    for metric, value in metric_result.items():\n",
    "        if metric=='accuracy':\n",
    "            global_metrics[task]['acc'].append(value) \n",
    "        else:\n",
    "            for g,v in value.items():\n",
    "                global_metrics[task][metric][g].append(v)\n",
    "\n",
    "        print(metric,metric_result[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "save_path=f'm1_{model_name}_simple_{task}_3'\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay=0.1)\n",
    "p, c, epochs = train.train_function(model,num_epochs,dl_train[task],optimizer,early_stop=10,dl_val=dl_val[task],save_path=save_path,es_threshold=0)\n",
    "torch.save(p,save_path+'_probs'+'.pt')\n",
    "torch.save(c,save_path+'_corr'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.get_datamap_complete_graph(p,correctness_vector=c,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrnn.SingleTaskRRNN(vocab_size = vocab_size,lstm_hidden_dim = lstm_hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n')\n",
    "    print(f\"Evaluación en el conjunto el conjunto {task}\")\n",
    "    metric_result = eval_func(model,dl_eval[task])\n",
    "    for metric, value in metric_result.items():\n",
    "        if metric=='accuracy':\n",
    "            global_metrics[task]['acc'].append(value) \n",
    "        else:\n",
    "            for g,v in value.items():\n",
    "                global_metrics[task][metric][g].append(v)\n",
    "\n",
    "        print(metric,metric_result[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['f1','recall','precision']:\n",
    "    print(f'{metric} medio de los 3 modelos: \\n')\n",
    "    for task in ['about','to','as']:\n",
    "        print(task.upper())\n",
    "        print(f'Resultado global {metric}:',mean(global_metrics[task][metric]['average']))\n",
    "        print(f'{metric} etiqueta male:',mean(global_metrics[task][metric]['male']))\n",
    "        print(f'{metric} etiqueta female: ',mean(global_metrics[task][metric]['female']))\n",
    "        print('\\n')\n",
    "\n",
    "print(f'Accuracy medio de los 3 modelos: \\n')\n",
    "for task in ['about','to','as']:\n",
    "    print('\\n',task.upper())\n",
    "    print('Resultado global accuracy:',mean(global_metrics[task]['acc']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
