{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_metric\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding,AutoModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import AdamW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga y preprocesado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer que utilizaremos y función auxiliar para aplicarlo.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(item):\n",
    "    '''Aplica el tokenizer al elemento dado. Trunca su longitud a la \n",
    "    longitud máxima de DistilBert.\n",
    "    '''\n",
    "    return tokenizer(item[\"text\"],truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset md_gender_bias (C:\\Users\\kuina\\.cache\\huggingface\\datasets\\md_gender_bias\\wizard\\1.0.0\\8ae77b51acf93383161cc954b146159291beca6c979b54ce228c46db86116c05)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ac8ef8cae5425eab92d7570321b27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset('md_gender_bias','wizard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separo test y validación. Creo datos artificiales para las etiquetas que faltan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = data[\"train\"]\n",
    "about = train[\"gender\"]\n",
    "self = [np.random.choice(np.unique(about)) for i in range(len(about))]\n",
    "partner = [np.random.choice(np.unique(about)) for i in range(len(about))]\n",
    "\n",
    "\n",
    "eval = data[\"validation\"]\n",
    "aboutE = eval[\"gender\"]\n",
    "selfE = [np.random.choice(np.unique(aboutE)) for i in range(len(aboutE))]\n",
    "partnerE = [np.random.choice(np.unique(aboutE)) for i in range(len(aboutE))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = [ ( about[i] , self[i] , partner[i] ) for i in range(len(about))]\n",
    "labels_eval = [ ( aboutE[i] , selfE[i] , partnerE[i] ) for i in range(len(aboutE))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\kuina\\.cache\\huggingface\\datasets\\md_gender_bias\\wizard\\1.0.0\\8ae77b51acf93383161cc954b146159291beca6c979b54ce228c46db86116c05\\cache-866d8be03765ca0b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\kuina\\.cache\\huggingface\\datasets\\md_gender_bias\\wizard\\1.0.0\\8ae77b51acf93383161cc954b146159291beca6c979b54ce228c46db86116c05\\cache-db98a93f495d1c0e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8752eadf515a4a9a8000d3b6d6e7907d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_data = data.map(preprocess_function,batched=True)\n",
    "token_data = token_data.remove_columns([\"chosen_topic\",\"text\",\"gender\"])\n",
    "token_data= token_data.with_format(\"torch\")\n",
    "\n",
    "\n",
    "token_train = token_data[\"train\"]\n",
    "token_eval = token_data[\"validation\"]\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un dataset a partir de los datos iniciales.\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self,data,labels,attention_mask):\n",
    "        self.data = data\n",
    "        self.labels =labels \n",
    "        self.attention = attention_mask\n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "            \n",
    "        x = self.data[index]\n",
    "        labels =  self.labels[index]\n",
    "        labels = np.array([labels])\n",
    "\n",
    "        labels = labels\n",
    "        attention = self.attention[index]\n",
    "\n",
    "\n",
    "        sample = {'input_ids': x,\n",
    "                'attention_mask': attention,\n",
    "                'labels': torch.from_numpy(labels).view(-1,3)}\n",
    "\n",
    "        return  sample\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo los dataloaders con ambos conjuntos. Aplico padding de forma dinámica haciendo uso del DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = MyDataSet(token_train['input_ids'],labels_train,token_train['attention_mask'])\n",
    "evalData = MyDataSet(token_eval['input_ids'],labels_eval,token_eval['attention_mask'])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(trainData,batch_size=16,shuffle=True,collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(evalData,batch_size=16,shuffle=True,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset md_gender_bias (C:\\Users\\kuina\\.cache\\huggingface\\datasets\\md_gender_bias\\new_data\\1.0.0\\8ae77b51acf93383161cc954b146159291beca6c979b54ce228c46db86116c05)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af04f07ac254c4895d226fda2064057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data = load_dataset('md_gender_bias','new_data')\n",
    "tasks_names = new_data['train'].features['class_type'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_names = ['about','partner','self'] # Usar esto en casos que no cargue new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi(nn.Module):\n",
    "    def __init__(self,name,num_labels,tasks_names,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.encoder = AutoModel.from_pretrained(name,num_labels=num_labels,output_attentions=True,output_hidden_states = True)\n",
    "        self.taskLayer = nn.ModuleList([])\n",
    "        self.taskLayer.append(nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(768,num_labels)\n",
    "        ))\n",
    "\n",
    "        # Task 0: About\n",
    "        # Task 1: Self\n",
    "        # Task 2: Partner\n",
    "        self.tasksname = {v:k for v,k in enumerate(tasks_names)}\n",
    "        # self.taskstrace = {v : [] for v in range(len(self.tasksname)) }\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self,input_ids = None,attention_mask = None,labels = None,output_attentions=None,output_hidden_states=None):\n",
    "        \n",
    "\n",
    "\n",
    "        dBertoutputs = self.encoder(input_ids,attention_mask = attention_mask,output_attentions=output_attentions,output_hidden_states = output_hidden_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        outputs_last_hidden_state = dBertoutputs[0]\n",
    "\n",
    "        cls_out = outputs_last_hidden_state[:,0]\n",
    "\n",
    "        tasks_output = {v : cls_out.clone() for v in self.tasksname.keys()}\n",
    "\n",
    "        for layer in self.taskLayer:\n",
    "            tasks_output = {v: layer(k) for v,k in tasks_output.items()}\n",
    "\n",
    "            # self.taskstrace = {v: tasks_output[v] for v in self.taskstrace.keys()}\n",
    "\n",
    "\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "            task_loss = [loss_fct(tasks_output[i] , labels[:,0][:,i]) for i in range(len(tasks_output))]\n",
    "\n",
    "\n",
    "            loss = sum(task_loss)\n",
    "\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=tasks_output, hidden_states=dBertoutputs.hidden_states,attentions=dBertoutputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Multi(\"distilbert-base-uncased\", 3,tasks_names).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7669943d1544a1aab03b80a7b17d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 654/1962 [00:13<00:20, 64.48it/s]\n",
      " 67%|██████▋   | 1308/1962 [00:22<00:09, 70.91it/s]\n",
      "100%|██████████| 1962/1962 [00:32<00:00, 67.77it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "train_function(model,num_epochs,train_dataloader,optimizer = AdamW(model.parameters(),lr=5e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44584a77a7bb42bb81afb3f203d8fbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 654/1962 [00:43<01:25, 15.32it/s]\n",
      " 67%|██████▋   | 1308/1962 [01:27<00:41, 15.85it/s]\n",
      "100%|██████████| 1962/1962 [02:10<00:00, 15.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tras modificar\n",
    "num_epochs = 3\n",
    "train_function(model,num_epochs,train_dataloader,optimizer = AdamW(model.parameters(),lr=5e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function_multi_single(model,metric,eval_dataloader,task = 0):\n",
    "    '''Función para evaluar un modelo dado en la tarea indicada.\n",
    "    \n",
    "    model: modelo a evaluar\n",
    "    metric: metrica a utilizar en la evaluación.\n",
    "    eval_dataloader: conjunto para evaluar. Debe ser un Pytorch DataLoader. \n",
    "    task: tarea en la que queremos evaluar el modelo'''\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        # logits = outputs.logits \n",
    "        logits = outputs.logits[task] \n",
    "        predictions = torch.argmax(logits,dim=-1)\n",
    "        metric.add_batch(predictions = predictions, references = batch[\"labels\"][:,0][:,task])\n",
    "\n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos en la tarea 0 (about)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8081936685288641}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_function_multi_single(model,metric,eval_dataloader,0)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 1 (self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.3500931098696462}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_function_multi_single(model,metric,eval_dataloader,1) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 2 (partner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.3258845437616387}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_function_multi_single(model,metric,eval_dataloader,2)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function_multi(model,eval_dataloader,tasks_names):\n",
    "    '''Función para evaluar un modelo dado en la tarea indicada.\n",
    "    \n",
    "    model: modelo a evaluar\n",
    "    \n",
    "    eval_dataloader: conjunto para evaluar. Debe ser un Pytorch DataLoader. \n",
    "    task_names: tareas en las que voy a evaluar mi modelo'''\n",
    "\n",
    "    # Usa accuracy, puedo cambiar para pasarle en la entrada la metrica (o métricas) que quiera utilizar!! (la paso suelta o paso una lista de metricas)\n",
    "    model.eval()\n",
    "    metrics = {task : load_metric(\"accuracy\") for task  in range(len(tasks_names))} # Se podria modificar en funcion del tipo de tarea\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        # logits = outputs.logits \n",
    "        logits = outputs.logits\n",
    "        predictions = {task : torch.argmax(logits[task],dim=-1) for task in range(len(tasks_names))}\n",
    "\n",
    "        for task, metric in metrics.items():\n",
    "            metric.add_batch(predictions = predictions[task], references = batch[\"labels\"][:,0][:,task])\n",
    "\n",
    "\n",
    "    return {tasks_names[task] : metric.compute() for task, metric in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about': {'accuracy': 0.8081936685288641},\n",
       " 'partner': {'accuracy': 0.3500931098696462},\n",
       " 'self': {'accuracy': 0.3258845437616387}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_function_multi(model,eval_dataloader,tasks_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in eval_dataloader:\n",
    "    batch = {k:v.to(device) for k,v in batch.items()}\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicciones en la etiqueta about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(logits[0],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 0, 2, 1, 2, 2, 2, 0, 2, 0, 2, 1, 2, 2], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 2, 0, 2, 1, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][:,0][:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pruebas new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'original', 'labels', 'class_type', 'turker_gender', 'episode_done', 'confidence'],\n",
       "        num_rows: 2345\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'original': Value(dtype='string', id=None),\n",
       " 'labels': [ClassLabel(num_classes=6, names=['ABOUT:female', 'ABOUT:male', 'PARTNER:female', 'PARTNER:male', 'SELF:female', 'SELF:male'], id=None)],\n",
       " 'class_type': ClassLabel(num_classes=3, names=['about', 'partner', 'self'], id=None),\n",
       " 'turker_gender': ClassLabel(num_classes=5, names=['man', 'woman', 'nonbinary', 'prefer not to say', 'no answer'], id=None),\n",
       " 'episode_done': Value(dtype='bool_', id=None),\n",
       " 'confidence': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_label = 0\n",
    "multi_label = 0\n",
    "for item in new_data['train']['labels']:\n",
    "    if len(item) == 1:\n",
    "        one_label +=1\n",
    "    else:\n",
    "        multi_label +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos con una etiqueta:  2345\n",
      "Ejemplos con varias etiquetas:  0\n"
     ]
    }
   ],
   "source": [
    "print('Ejemplos con una etiqueta: ', one_label)\n",
    "print('Ejemplos con varias etiquetas: ' , multi_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tfg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35e8332e99bdf485583869dfbdef293dcf2f9293b1663ec5daea0a573af457c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
